Tu es une IA experte en dÃ©veloppement dâ€™applications locales complÃ¨tes avec des outils open source. Tu vas concevoir une solution locale pour automatiser la recherche d'emploi d'un utilisateur. Voici les exigences prÃ©cises du projet :

ğŸ”§ Objectif gÃ©nÃ©ral :

CrÃ©er une application desktop locale (via Electron, Tauri ou Ã©quivalent) avec une interface utilisateur en mode Kanban (intÃ©gration avec NocoDB ou UI Ã©quivalente) pour gÃ©rer la recherche dâ€™emploi.
Tous les traitements doivent se faire localement : pas dâ€™appel Ã  des API externes, uniquement du scraping et du traitement offline avec des outils open source, installables en local via Docker si nÃ©cessaire.

ğŸ§  Connaissance du profil utilisateur :

Le systÃ¨me connaÃ®t le profil de lâ€™utilisateur via un fichier de configuration YAML ou JSON (ex. : prÃ©fÃ©rences, localisation, secteurs visÃ©s, mobilitÃ© gÃ©ographique, types de contrats souhaitÃ©s).

Le systÃ¨me extrait automatiquement les donnÃ©es du CV au format PDF (parsing sÃ©mantique + structuration) pour en extraire : expÃ©riences, compÃ©tences, formations, soft skills, etc.

ğŸ” Scraping intelligent & recherche dâ€™offres :

GÃ©nÃ¨re automatiquement des mots-clÃ©s de recherche dâ€™emploi Ã  partir du profil et du CV (extraction sÃ©mantique).

Scrape les sites suivants de maniÃ¨re intelligente :

LinkedIn

Indeed

Apec

(et tout autre site franÃ§ais populaire si pertinent)

Utilise Playwright ou Selenium pour le scraping dynamique. Les rÃ©sultats doivent Ãªtre stockÃ©s dans une base locale (SQLite ou Ã©quivalent).

ğŸ¯ Matching, scoring et filtrage :

Calcule un score de matching (de 0 Ã  100) basÃ© sur la similaritÃ© entre lâ€™annonce et le profil de lâ€™utilisateur :

Matching sÃ©mantique via embeddings (LLM local ou Sentence Transformers)

PondÃ©ration basÃ©e sur les critÃ¨res de prÃ©fÃ©rence utilisateur (distance, niveau d'expÃ©rience, salaire, etc.)

Filtrage avancÃ© des offres :

Temps de transport : Pour chaque offre, estime le temps de transport entre le domicile de l'utilisateur et le lieu de l'offre (via gÃ©olocalisation et calcul d'itinÃ©raire). Si le temps de transport est supÃ©rieur Ã  1 heure, l'offre est automatiquement filtrÃ©e.

Salaire : Si le salaire est mentionnÃ© dans l'offre et qu'il est infÃ©rieur Ã  50Kâ‚¬, filtre l'offre. Seules les offres avec un salaire Ã©gal ou supÃ©rieur Ã  50Kâ‚¬ ou sans information salariale explicite sont conservÃ©es.

ğŸ§© Interface Kanban - NocoDB :

Les annonces sont affichÃ©es dans une interface type Kanban, avec les colonnes suivantes :

Backlog

To Be Reviewed

For Application

Applied

Rejected by me

Negative Answer

Chaque carte doit contenir :

Lâ€™annonce (titre, entreprise, lieu, lien, description complÃ¨te, date)

Le score de matching

Les tags mÃ©tiers / compÃ©tences

Champs personnalisables (notes, intÃ©rÃªt, deadline, etc.)

ğŸ“„ GÃ©nÃ©ration de contenus automatisÃ©e :

Pour chaque carte dÃ©placÃ©e vers For Application :

GÃ©nÃ¨re une lettre de motivation personnalisÃ©e en .docx, basÃ©e sur :

Lâ€™annonce

Le poste et lâ€™entreprise

Le profil utilisateur et son parcours

GÃ©nÃ¨re un CV personnalisÃ© avec des recommandations d'amÃ©lioration et crÃ©e un nouveau fichier .docx prÃªt Ã  lâ€™envoi.

Utilise un LLM local (ex : LLaMA 3.2 via Ollama) et des templates de prompt prÃ©dÃ©finis pour ces tÃ¢ches.

ğŸ“¦ Contraintes techniques & architecture attendue :

Tout est local, sans appel dâ€™API externes.

Utilise uniquement des outils open source (Python, Node.js, Docker, Ollama, NocoDB, SQLiteâ€¦).

Lâ€™interface peut Ãªtre dÃ©veloppÃ©e avec :

Tauri (Rust + frontend web)

Electron.js

Ou une webapp locale (ex. Flask + React/Tailwind)

Propose une architecture modulaire, facilement extensible et maintenable.

ğŸ“ˆ Bonus et amÃ©liorations supplÃ©mentaires :

Mise en place dâ€™un scheduler local (ex. avec cron) pour lancer les scrapes Ã  intervalles rÃ©guliers (quotidiens ou hebdomadaires).

IntÃ©gration dâ€™une boucle de feedback pour amÃ©liorer le scoring : lorsque des offres sont rejetÃ©es, possibilitÃ© dâ€™entraÃ®ner un modÃ¨le local pour affiner les critÃ¨res de matching.

IntÃ©gration possible de Haystack + Qdrant pour des recherches sÃ©mantiques avancÃ©es dans la base dâ€™annonces.

ImplÃ©mentation dâ€™un plugin de scraping configurable par YAML pour ajouter de nouveaux sites facilement.

GÃ©nÃ©ration dâ€™un journal de bord exportable en PDF pour suivre lâ€™historique des candidatures et les statistiques de matching.

âœ… Livrables attendus :

SchÃ©ma dâ€™architecture dÃ©taillÃ©

Stack technique et scripts de setup Docker

Design du schÃ©ma de donnÃ©es

Code ou pseudo-code des modules clÃ©s

JSON de la structure NocoDB (si utilisÃ©)

Templates pour les prompts de lettre de motivation et CV

Instructions supplÃ©mentaires :

Documente chaque Ã©tape du dÃ©veloppement et ajoute des commentaires dÃ©taillÃ©s.

Fournis des idÃ©es et propositions dâ€™amÃ©lioration Ã  chaque module (scraping, matching, filtrage, interface Kanban, gÃ©nÃ©ration de documents).

Assure la modularitÃ© et la possibilitÃ© dâ€™Ã©tendre ou dâ€™intÃ©grer facilement dâ€™autres fonctionnalitÃ©s ultÃ©rieurement.

ğŸ’¡ Propositions dâ€™amÃ©lioration supplÃ©mentaires :
Scheduler local pour automatiser le scraping Ã  des intervalles rÃ©guliers.

Feedback loop : Utilisation des retours (ex. refus, taux de conversion) pour ajuster automatiquement le scoring.

Recherche sÃ©mantique avancÃ©e en intÃ©grant Haystack + Qdrant pour des filtres plus prÃ©cis.

Plugin configurable : Permettre l'ajout de nouveaux sites de scraping via un fichier de configuration YAML.

Tableau de bord analytique pour visualiser les KPIs (nombre d'offres scrappÃ©es, taux de matching, taux de conversion des candidatures, etc.).

CLI pour lancer manuellement ou planifier des actions spÃ©cifiques (scraping, scoring, gÃ©nÃ©ration de documents).